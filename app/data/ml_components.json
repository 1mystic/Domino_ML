[
  {
    "id": "csv-loader",
    "name": "CSV Loader",
    "category": "Data Sources",
    "type": "data",
    "description": "Load data from CSV file",
    "icon": "üìä",
    "parameters": [
      {
        "name": "file_path",
        "type": "string",
        "label": "File Path",
        "description": "Path to the CSV file",
        "defaultValue": "data.csv",
        "required": true
      },
      {
        "name": "separator",
        "type": "select",
        "label": "Separator",
        "defaultValue": ",",
        "options": [
          ",",
          ";",
          "\\t",
          "|"
        ]
      }
    ],
    "inputs": [],
    "outputs": [
      "data"
    ],
    "pythonTemplate": "\nimport pandas as pd\n\n# Load data from CSV\ndata = pd.read_csv('{file_path}', sep='{separator}')\nprint(f\"Loaded dataset with shape: {data.shape}\")\n"
  },
  {
    "id": "sample-data",
    "name": "Sample Dataset",
    "category": "Data Sources",
    "type": "data",
    "description": "Load built-in sample datasets",
    "icon": "üéØ",
    "parameters": [
      {
        "name": "dataset",
        "type": "select",
        "label": "Dataset",
        "defaultValue": "iris",
        "options": [
          "iris",
          "wine",
          "breast_cancer",
          "digits"
        ]
      }
    ],
    "inputs": [],
    "outputs": [
      "data"
    ],
    "pythonTemplate": "\nfrom sklearn.datasets import load_{dataset}\nimport pandas as pd\n\n# Load sample dataset\ndataset = load_{dataset}()\ndata = pd.DataFrame(dataset.data, columns=dataset.feature_names)\nif hasattr(dataset, 'target'):\n    data['target'] = dataset.target\nprint(f\"Loaded dataset with shape: {data.shape}\")\n"
  },
  {
    "id": "standard-scaler",
    "name": "Standard Scaler",
    "category": "Preprocessing",
    "type": "preprocessing",
    "description": "Standardize features by removing mean and scaling to unit variance",
    "icon": "üìè",
    "parameters": [
      {
        "name": "with_mean",
        "type": "boolean",
        "label": "Center Data",
        "defaultValue": true
      },
      {
        "name": "with_std",
        "type": "boolean",
        "label": "Scale Data",
        "defaultValue": true
      }
    ],
    "inputs": [
      "data"
    ],
    "outputs": [
      "scaled_data"
    ],
    "pythonTemplate": "\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize and fit the scaler\nscaler = StandardScaler(with_mean={with_mean}, with_std={with_std})\nscaled_data = scaler.fit_transform(data)\nprint(f\"Applied StandardScaler to data with shape: {scaled_data.shape}\")\n"
  },
  {
    "id": "train-test-split",
    "name": "Train Test Split",
    "category": "Preprocessing",
    "type": "preprocessing",
    "description": "Split data into training and testing sets",
    "icon": "‚úÇÔ∏è",
    "parameters": [
      {
        "name": "test_size",
        "type": "number",
        "label": "Test Size",
        "defaultValue": 0.2,
        "min": 0.1,
        "max": 0.9,
        "step": 0.05
      },
      {
        "name": "random_state",
        "type": "number",
        "label": "Random State",
        "defaultValue": 42
      }
    ],
    "inputs": [
      "data"
    ],
    "outputs": [
      "X_train",
      "X_test",
      "y_train",
      "y_test"
    ],
    "pythonTemplate": "\nfrom sklearn.model_selection import train_test_split\n\n# Separate features and target\nX = data.drop('target', axis=1)\ny = data['target']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size={test_size}, random_state={random_state}\n)\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Test set size: {X_test.shape[0]}\")\n"
  },
  {
    "id": "min-max-scaler",
    "name": "Min-Max Scaler",
    "category": "Preprocessing",
    "type": "preprocessing",
    "description": "Scale features to a fixed range, typically [0, 1]",
    "icon": "üìê",
    "parameters": [
      {
        "name": "feature_range_min",
        "type": "number",
        "label": "Range Min",
        "defaultValue": 0,
        "min": -10,
        "max": 10,
        "step": 0.1
      },
      {
        "name": "feature_range_max",
        "type": "number",
        "label": "Range Max",
        "defaultValue": 1,
        "min": -10,
        "max": 10,
        "step": 0.1
      }
    ],
    "inputs": [
      "data"
    ],
    "outputs": [
      "scaled_data"
    ],
    "pythonTemplate": "\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize and fit the scaler\nscaler = MinMaxScaler(feature_range=({feature_range_min}, {feature_range_max}))\nscaled_data = scaler.fit_transform(data)\nprint(f\"Applied MinMaxScaler to data with shape: {scaled_data.shape}\")\n"
  },
  {
    "id": "pca",
    "name": "PCA Dimensionality Reduction",
    "category": "Preprocessing",
    "type": "preprocessing",
    "description": "Principal Component Analysis for dimensionality reduction",
    "icon": "üìä",
    "parameters": [
      {
        "name": "n_components",
        "type": "number",
        "label": "Components",
        "defaultValue": 2,
        "min": 1,
        "max": 50
      },
      {
        "name": "whiten",
        "type": "boolean",
        "label": "Whiten Components",
        "defaultValue": false
      }
    ],
    "inputs": [
      "data"
    ],
    "outputs": [
      "transformed_data"
    ],
    "pythonTemplate": "\nfrom sklearn.decomposition import PCA\n\n# Initialize and fit PCA\npca = PCA(n_components={n_components}, whiten={whiten})\ntransformed_data = pca.fit_transform(data)\nprint(f\"PCA reduced data to {transformed_data.shape[1]} components\")\nprint(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n"
  },
  {
    "id": "random-forest-classifier",
    "name": "Random Forest Classifier",
    "category": "Classification",
    "type": "model",
    "description": "Random Forest classification algorithm",
    "icon": "üå≤",
    "parameters": [
      {
        "name": "n_estimators",
        "type": "number",
        "label": "Number of Trees",
        "defaultValue": 100,
        "min": 1,
        "max": 1000
      },
      {
        "name": "max_depth",
        "type": "number",
        "label": "Max Depth",
        "defaultValue": 10,
        "min": 1,
        "max": 50
      },
      {
        "name": "random_state",
        "type": "number",
        "label": "Random State",
        "defaultValue": 42
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize and train the model\nmodel = RandomForestClassifier(\n    n_estimators={n_estimators},\n    max_depth={max_depth},\n    random_state={random_state}\n)\nmodel.fit(X_train, y_train)\nprint(\"Random Forest Classifier trained successfully\")\n"
  },
  {
    "id": "svm-classifier",
    "name": "SVM Classifier",
    "category": "Classification",
    "type": "model",
    "description": "Support Vector Machine classifier",
    "icon": "‚ö°",
    "parameters": [
      {
        "name": "kernel",
        "type": "select",
        "label": "Kernel",
        "defaultValue": "rbf",
        "options": [
          "linear",
          "poly",
          "rbf",
          "sigmoid"
        ]
      },
      {
        "name": "C",
        "type": "number",
        "label": "Regularization (C)",
        "defaultValue": 1.0,
        "min": 0.01,
        "max": 100,
        "step": 0.01
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.svm import SVC\n\n# Initialize and train the model\nmodel = SVC(kernel='{kernel}', C={C}, random_state=42)\nmodel.fit(X_train, y_train)\nprint(\"SVM Classifier trained successfully\")\n"
  },
  {
    "id": "logistic-regression",
    "name": "Logistic Regression",
    "category": "Classification",
    "type": "model",
    "description": "Linear classifier for binary and multiclass problems",
    "icon": "üìà",
    "parameters": [
      {
        "name": "C",
        "type": "number",
        "label": "Regularization (C)",
        "defaultValue": 1.0,
        "min": 0.01,
        "max": 100,
        "step": 0.01
      },
      {
        "name": "max_iter",
        "type": "number",
        "label": "Max Iterations",
        "defaultValue": 1000,
        "min": 100,
        "max": 5000,
        "step": 100
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.linear_model import LogisticRegression\n\n# Initialize and train the model\nmodel = LogisticRegression(C={C}, max_iter={max_iter}, random_state=42)\nmodel.fit(X_train, y_train)\nprint(\"Logistic Regression model trained successfully\")\n"
  },
  {
    "id": "knn-classifier",
    "name": "K-Nearest Neighbors",
    "category": "Classification",
    "type": "model",
    "description": "Instance-based learning algorithm",
    "icon": "üë•",
    "parameters": [
      {
        "name": "n_neighbors",
        "type": "number",
        "label": "Number of Neighbors",
        "defaultValue": 5,
        "min": 1,
        "max": 50
      },
      {
        "name": "weights",
        "type": "select",
        "label": "Weights",
        "defaultValue": "uniform",
        "options": [
          "uniform",
          "distance"
        ]
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Initialize and train the model\nmodel = KNeighborsClassifier(\n    n_neighbors={n_neighbors},\n    weights='{weights}'\n)\nmodel.fit(X_train, y_train)\nprint(\"K-Nearest Neighbors Classifier trained successfully\")\n"
  },
  {
    "id": "linear-regression",
    "name": "Linear Regression",
    "category": "Regression",
    "type": "model",
    "description": "Linear regression algorithm",
    "icon": "üìà",
    "parameters": [
      {
        "name": "fit_intercept",
        "type": "boolean",
        "label": "Fit Intercept",
        "defaultValue": true
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.linear_model import LinearRegression\n\n# Initialize and train the model\nmodel = LinearRegression(fit_intercept={fit_intercept})\nmodel.fit(X_train, y_train)\nprint(\"Linear Regression model trained successfully\")\n"
  },
  {
    "id": "random-forest-regressor",
    "name": "Random Forest Regressor",
    "category": "Regression",
    "type": "model",
    "description": "Random Forest regression algorithm",
    "icon": "üå≥",
    "parameters": [
      {
        "name": "n_estimators",
        "type": "number",
        "label": "Number of Trees",
        "defaultValue": 100,
        "min": 1,
        "max": 1000
      },
      {
        "name": "max_depth",
        "type": "number",
        "label": "Max Depth",
        "defaultValue": 10,
        "min": 1,
        "max": 50
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Initialize and train the model\nmodel = RandomForestRegressor(\n    n_estimators={n_estimators},\n    max_depth={max_depth},\n    random_state=42\n)\nmodel.fit(X_train, y_train)\nprint(\"Random Forest Regressor trained successfully\")\n"
  },
  {
    "id": "classification-metrics",
    "name": "Classification Metrics",
    "category": "Evaluation",
    "type": "evaluation",
    "description": "Calculate classification performance metrics",
    "icon": "üìä",
    "parameters": [],
    "inputs": [
      "model",
      "X_test",
      "y_test"
    ],
    "outputs": [
      "metrics"
    ],
    "pythonTemplate": "\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test, y_pred))\n"
  },
  {
    "id": "regression-metrics",
    "name": "Regression Metrics",
    "category": "Evaluation",
    "type": "evaluation",
    "description": "Calculate regression performance metrics",
    "icon": "üìà",
    "parameters": [],
    "inputs": [
      "model",
      "X_test",
      "y_test"
    ],
    "outputs": [
      "metrics"
    ],
    "pythonTemplate": "\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Calculate metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"Root Mean Squared Error: {rmse:.4f}\")\nprint(f\"Mean Absolute Error: {mae:.4f}\")\nprint(f\"R¬≤ Score: {r2:.4f}\")\n"
  },
  {
    "id": "kmeans-clustering",
    "name": "K-Means Clustering",
    "category": "Clustering",
    "type": "model",
    "description": "Partition data into K clusters",
    "icon": "üéØ",
    "parameters": [
      {
        "name": "n_clusters",
        "type": "number",
        "label": "Number of Clusters",
        "defaultValue": 3,
        "min": 2,
        "max": 20
      }
    ],
    "inputs": [
      "data"
    ],
    "outputs": [
      "labels",
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.cluster import KMeans\n\n# Initialize and fit the model\nmodel = KMeans(n_clusters={n_clusters}, random_state=42)\nlabels = model.fit_predict(data)\nprint(f\"K-Means clustering completed with {len(set(labels))} clusters\")\n"
  },
  {
    "id": "one-hot-encoder",
    "name": "One-Hot Encoder",
    "category": "Preprocessing",
    "type": "preprocessing",
    "description": "Encode categorical features as a one-hot numeric array",
    "icon": "üî¢",
    "parameters": [
      {
        "name": "handle_unknown",
        "type": "select",
        "label": "Handle Unknown",
        "defaultValue": "error",
        "options": [
          "error",
          "ignore"
        ]
      },
      {
        "name": "sparse",
        "type": "boolean",
        "label": "Sparse Output",
        "defaultValue": true
      }
    ],
    "inputs": [
      "data"
    ],
    "outputs": [
      "encoded_data"
    ],
    "pythonTemplate": "\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Initialize and fit the encoder\nencoder = OneHotEncoder(handle_unknown='{handle_unknown}', sparse={sparse})\nencoded_data = encoder.fit_transform(data)\nprint(f\"One-Hot Encoded data shape: {encoded_data.shape}\")\n"
  },
  {
    "id": "tfidf-vectorizer",
    "name": "TF-IDF Vectorizer",
    "category": "Preprocessing",
    "type": "preprocessing",
    "description": "Convert a collection of raw documents to a matrix of TF-IDF features",
    "icon": "üìù",
    "parameters": [
      {
        "name": "max_features",
        "type": "number",
        "label": "Max Features",
        "defaultValue": 1000,
        "min": 100,
        "max": 10000,
        "step": 100
      },
      {
        "name": "stop_words",
        "type": "select",
        "label": "Stop Words",
        "defaultValue": "english",
        "options": [
          "english",
          "none"
        ]
      }
    ],
    "inputs": [
      "data"
    ],
    "outputs": [
      "tfidf_matrix"
    ],
    "pythonTemplate": "\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize and fit the vectorizer\nstop_words = '{stop_words}' if '{stop_words}' != 'none' else None\nvectorizer = TfidfVectorizer(max_features={max_features}, stop_words=stop_words)\ntfidf_matrix = vectorizer.fit_transform(data)\nprint(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")\n"
  },
  {
    "id": "gradient-boosting-classifier",
    "name": "Gradient Boosting Classifier",
    "category": "Classification",
    "type": "model",
    "description": "Gradient Boosting for classification",
    "icon": "üöÄ",
    "parameters": [
      {
        "name": "n_estimators",
        "type": "number",
        "label": "Number of Trees",
        "defaultValue": 100,
        "min": 10,
        "max": 1000
      },
      {
        "name": "learning_rate",
        "type": "number",
        "label": "Learning Rate",
        "defaultValue": 0.1,
        "min": 0.001,
        "max": 1.0,
        "step": 0.01
      },
      {
        "name": "max_depth",
        "type": "number",
        "label": "Max Depth",
        "defaultValue": 3,
        "min": 1,
        "max": 20
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize and train the model\nmodel = GradientBoostingClassifier(\n    n_estimators={n_estimators},\n    learning_rate={learning_rate},\n    max_depth={max_depth},\n    random_state=42\n)\nmodel.fit(X_train, y_train)\nprint(\"Gradient Boosting Classifier trained successfully\")\n"
  },
  {
    "id": "mlp-classifier",
    "name": "MLP Neural Network",
    "category": "Classification",
    "type": "model",
    "description": "Multi-layer Perceptron classifier",
    "icon": "üß†",
    "parameters": [
      {
        "name": "hidden_layer_sizes",
        "type": "string",
        "label": "Hidden Layers (e.g. 100,50)",
        "defaultValue": "100",
        "description": "Comma-separated list of neuron counts"
      },
      {
        "name": "activation",
        "type": "select",
        "label": "Activation",
        "defaultValue": "relu",
        "options": [
          "identity",
          "logistic",
          "tanh",
          "relu"
        ]
      },
      {
        "name": "max_iter",
        "type": "number",
        "label": "Max Iterations",
        "defaultValue": 200,
        "min": 50,
        "max": 2000
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.neural_network import MLPClassifier\n\n# Parse hidden layers\nhidden_layers = tuple(map(int, '{hidden_layer_sizes}'.split(',')))\n\n# Initialize and train the model\nmodel = MLPClassifier(\n    hidden_layer_sizes=hidden_layers,\n    activation='{activation}',\n    max_iter={max_iter},\n    random_state=42\n)\nmodel.fit(X_train, y_train)\nprint(\"MLP Classifier trained successfully\")\n"
  },
  {
    "id": "confusion-matrix",
    "name": "Confusion Matrix",
    "category": "Evaluation",
    "type": "evaluation",
    "description": "Compute confusion matrix to evaluate the accuracy of a classification",
    "icon": "‚¨õ",
    "parameters": [],
    "inputs": [
      "model",
      "X_test",
      "y_test"
    ],
    "outputs": [
      "matrix_plot"
    ],
    "pythonTemplate": "\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.title(\"Confusion Matrix\")\nplt.show()\n"
  },
  {
    "id": "text-loader",
    "name": "Text Loader",
    "category": "Data Sources",
    "type": "data",
    "description": "Load text data from directory or file",
    "icon": "üìù",
    "parameters": [
      {
        "name": "path",
        "type": "string",
        "label": "Data Location",
        "description": "Path to text files",
        "defaultValue": "./data/text",
        "required": true
      }
    ],
    "inputs": [],
    "outputs": [
      "data",
      "target"
    ],
    "pythonTemplate": "\nfrom sklearn.datasets import load_files\n\n# Load text data\ndataset = load_files('{path}')\ndata, target = dataset.data, dataset.target\nprint(f\"Loaded {len(data)} text documents\")\n"
  },
  {
    "id": "label-encoder",
    "name": "Label Encoder",
    "category": "Preprocessing",
    "type": "preprocessing",
    "description": "Encode target labels with value between 0 and n_classes-1",
    "icon": "üè∑Ô∏è",
    "parameters": [],
    "inputs": [
      "y"
    ],
    "outputs": [
      "y_encoded"
    ],
    "pythonTemplate": "\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\nprint(f\"Encoded {len(le.classes_)} classes\")\n"
  },
  {
    "id": "decision-tree-classifier",
    "name": "Decision Tree",
    "category": "Classification",
    "type": "model",
    "description": "Decision Tree Classifier",
    "icon": "üå≥",
    "parameters": [
      {
        "name": "max_depth",
        "type": "number",
        "label": "Max Depth",
        "defaultValue": 5,
        "min": 1,
        "max": 50
      },
      {
        "name": "criterion",
        "type": "select",
        "label": "Criterion",
        "defaultValue": "gini",
        "options": [
          "gini",
          "entropy"
        ]
      }
    ],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(max_depth={max_depth}, criterion='{criterion}', random_state=42)\nmodel.fit(X_train, y_train)\nprint(\"Decision Tree trained\")\n"
  },
  {
    "id": "naive-bayes",
    "name": "Naive Bayes",
    "category": "Classification",
    "type": "model",
    "description": "Gaussian Naive Bayes",
    "icon": "üîî",
    "parameters": [],
    "inputs": [
      "X_train",
      "y_train"
    ],
    "outputs": [
      "model"
    ],
    "pythonTemplate": "\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\nprint(\"Naive Bayes model trained\")\n"
  },
  {
    "id": "cross-validation",
    "name": "Cross Validation",
    "category": "Evaluation",
    "type": "evaluation",
    "description": "Evaluate model using cross-validation",
    "icon": "üîÑ",
    "parameters": [
      {
        "name": "cv",
        "type": "number",
        "label": "Folds (K)",
        "defaultValue": 5,
        "min": 2,
        "max": 10
      }
    ],
    "inputs": [
      "model",
      "X",
      "y"
    ],
    "outputs": [
      "scores"
    ],
    "pythonTemplate": "\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X, y, cv={cv})\nprint(f\"Cross-validation scores: {scores}\")\nprint(f\"Mean CV Score: {scores.mean():.4f}\")\n"
  }
]