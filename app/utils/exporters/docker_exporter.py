"""
Docker Exporter - Generates Docker containers for ML pipelines
"""
from datetime import datetime
from typing import Dict, List
from app.utils.exporters.python_exporter import PythonExporter
from app.utils.exporters.requirements_builder import RequirementsBuilder


class DockerExporter:
    """
    Exports ML pipelines as Docker containers with all dependencies.
    """
    
    def __init__(self):
        self.python_version = "3.10"
        
    def export(
        self,
        nodes: List[Dict],
        edges: List[Dict],
        pipeline_name: str,
        description: str = "",
        python_version: str = "3.10"
    ) -> Dict[str, str]:
        """
        Export pipeline as Docker container artifacts.
        
        Args:
            nodes: List of pipeline nodes
            edges: List of pipeline edges
            pipeline_name: Name of the pipeline
            description: Pipeline description
            python_version: Python version for Docker image
            
        Returns:
            Dictionary with Dockerfile, script, requirements, and docker-compose
        """
        self.python_version = python_version
        
        # Generate Python script
        python_export = PythonExporter.export_pipeline(
            nodes, edges, pipeline_name, description, include_cli=True
        )
        
        # Generate Dockerfile
        dockerfile = self._generate_dockerfile(
            python_export['filename'],
            pipeline_name
        )
        
        # Generate docker-compose.yml
        docker_compose = self._generate_docker_compose(pipeline_name)
        
        # Generate .dockerignore
        dockerignore = self._generate_dockerignore()
        
        # Generate README
        readme = self._generate_docker_readme(pipeline_name)
        
        return {
            'Dockerfile': dockerfile,
            'docker-compose.yml': docker_compose,
            '.dockerignore': dockerignore,
            'README.md': readme,
            'script': python_export['script'],
            'requirements': python_export['requirements'],
            'script_filename': python_export['filename']
        }
    
    def _generate_dockerfile(self, script_filename: str, pipeline_name: str) -> str:
        """Generate Dockerfile content."""
        return f"""# Dockerfile for {pipeline_name}
# Generated by DominoML on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

FROM python:{self.python_version}-slim

# Set metadata
LABEL maintainer="DominoML"
LABEL description="{pipeline_name}"

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    build-essential \\
    git \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first (for better caching)
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip && \\
    pip install --no-cache-dir -r requirements.txt

# Copy pipeline script
COPY {script_filename} .

# Create directories
RUN mkdir -p /app/data /app/output /app/models

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Set permissions
RUN chmod +x {script_filename}

# Expose any ports if needed (uncomment if running a service)
# EXPOSE 8000

# Health check (optional)
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
    CMD python -c "print('healthy')" || exit 1

# Run the pipeline
CMD ["python", "{script_filename}"]
"""
    
    def _generate_docker_compose(self, pipeline_name: str) -> str:
        """Generate docker-compose.yml content."""
        service_name = pipeline_name.lower().replace(' ', '-')
        
        return f"""version: '3.8'

services:
  {service_name}:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: {service_name}
    volumes:
      - ./data:/app/data:ro
      - ./output:/app/output
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
    # Uncomment to limit resources
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2'
    #       memory: 4G
    restart: unless-stopped
    # Uncomment for GPU support
    # runtime: nvidia
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
"""
    
    def _generate_dockerignore(self) -> str:
        """Generate .dockerignore content."""
        return """# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Git
.git/
.gitignore

# Documentation
*.md
!README.md

# Logs
*.log

# Data (usually want to mount, not copy)
data/
output/
models/
"""
    
    def _generate_docker_readme(self, pipeline_name: str) -> str:
        """Generate Docker deployment README."""
        service_name = pipeline_name.lower().replace(' ', '-')
        
        return f"""# {pipeline_name} - Docker Deployment

Generated by DominoML on {datetime.now().strftime('%Y-%m-%d')}

## Prerequisites

- Docker Engine 20.10+
- Docker Compose 2.0+
- (Optional) NVIDIA Docker for GPU support

## Quick Start

### 1. Build the image

```bash
docker-compose build
```

### 2. Run the pipeline

```bash
docker-compose up
```

### 3. Run with custom data

```bash
# Place your data in ./data directory
# Results will be saved to ./output directory
docker-compose up
```

## Manual Docker Commands

### Build

```bash
docker build -t {service_name}:latest .
```

### Run

```bash
docker run --rm \\
  -v $(pwd)/data:/app/data:ro \\
  -v $(pwd)/output:/app/output \\
  {service_name}:latest
```

### Interactive mode

```bash
docker run -it --rm {service_name}:latest /bin/bash
```

## Directory Structure

```
.
├── Dockerfile              # Container definition
├── docker-compose.yml      # Orchestration config
├── requirements.txt        # Python dependencies
├── {service_name}.py       # Pipeline script
├── data/                   # Input data (mounted)
├── output/                 # Results (mounted)
└── models/                 # Model files (mounted)
```

## Volume Mounts

- `./data` → `/app/data` (read-only) - Input datasets
- `./output` → `/app/output` (read-write) - Results and outputs
- `./models` → `/app/models` (read-write) - Trained models

## Environment Variables

You can set environment variables in `docker-compose.yml`:

```yaml
environment:
  - DEBUG=true
  - LOG_LEVEL=INFO
```

## Resource Limits

Uncomment the `deploy` section in `docker-compose.yml` to set CPU/memory limits:

```yaml
deploy:
  resources:
    limits:
      cpus: '2'
      memory: 4G
```

## GPU Support

For NVIDIA GPU support:

1. Install [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)
2. Uncomment GPU sections in `docker-compose.yml`
3. Rebuild and run

## Troubleshooting

### Permission issues

```bash
# Fix output directory permissions
chmod 777 output/
```

### Out of memory

Increase Docker memory limit in Docker Desktop settings or docker-compose.yml

### Image too large

Use multi-stage builds or alpine-based images for smaller size

## Production Deployment

For production, consider:

- Using specific version tags (not `latest`)
- Implementing health checks
- Setting up monitoring and logging
- Using container orchestration (Kubernetes, etc.)
- Implementing secrets management

## Support

For issues with the pipeline code, check the generated script.
For Docker-specific issues, check Docker logs:

```bash
docker-compose logs
```
"""
    
    @staticmethod
    def export_docker(
        nodes: List[Dict],
        edges: List[Dict],
        pipeline_name: str,
        description: str = "",
        python_version: str = "3.10"
    ) -> Dict[str, str]:
        """
        Convenience method to export Docker artifacts.
        
        Returns:
            Dictionary with all Docker-related files
        """
        exporter = DockerExporter()
        return exporter.export(nodes, edges, pipeline_name, description, python_version)
